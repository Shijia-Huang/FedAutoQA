# FedAutoQA

![Python](https://img.shields.io/badge/python-3.11%2B-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.111-green.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

FedAutoQA is a **retrievalâ€‘augmented QA microâ€‘service** that answers questions about HCUP (Healthcare Cost and Utilization Project) documentation.  
It combines semantic search (Sentenceâ€‘Transformersâ€¯+â€¯FAISS) with Googleâ€™s **GeminiÂ 2** models, wrapped in a lightweight FastAPI backend and an inâ€‘browser HTML interface.

---

## âœ¨ Key Features

- **Oneâ€‘click local UI** â€“ open `http://localhost:8000` and ask questions right away  
- **RAG pipeline** â€“ dense embedding search retrieves topâ€‘K FAQ chunks which are fed to Gemini with a strict system prompt  
- **Stateless API** â€“ single `/ask` POST endpoint; easy to drop behind any frontend  
- **Dockerâ€‘ready** â€“ run the whole stack with one `docker compose up`  
- **Configurable LLM** â€“ flip between `geminiâ€‘2.0â€‘flash` for speed or `geminiâ€‘2.0â€‘pro` for quality  
- **Test corpus** â€“ ships with ~2â€¯MB of HCUP FAQ text split into 300Â chunks for instant experimentation  

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart TD
    subgraph Client
      A[Browser UI] -->|JSON POST /ask| B
    end
    subgraph Server
      B[FastAPIÂ app] --> C[FAISSÂ index]
      B --> D[GeminiÂ API]
    end
    C -->|topâ€‘K context| D
    D -->|answer| B
```

---

## ğŸš€ QuickÂ Start

```bash
# 1. Clone
git clone https://github.com/KenSu223/FedAutoQA.git
cd FedAutoQA

# 2. Install deps (PythonÂ 3.11+)
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 3. Set your Gemini key
export GOOGLE_API_KEY="<YOUR_TOKEN>"           # or GEMINI_API_KEY

# 4. Fire it up
uvicorn api:app --reload

# 5. Open the UI
open http://localhost:8000/docs
```

### Using Docker

```bash
docker compose up --build
```

---

## âš™ï¸ Configuration

| Variable            | Purpose                                | Default               |
|---------------------|----------------------------------------|-----------------------|
| `GOOGLE_API_KEY`    | Your Gemini API key                    | _required_            |
| `SIM_THRESHOLD`     | Min cosine similarity for retrieved chunk | `0.7`                 |
| `TOP_K`             | Number of context chunks to send to LLM | `5`                   |
| `MODEL_NAME`        | Gemini model (`gemini-2.0-flash`/`-pro`) | `gemini-2.0-flash`    |

Update values in **`api.py`** or via environment variables.

---

## ğŸ“‘ API Reference

| Method | Route | Body (JSON)           | Response (JSON)                |
|--------|-------|-----------------------|--------------------------------|
| `POST` | `/ask`| `{ "query": "..." }`  | `{ "answer": "...", "context": [...] }` |

---

## ğŸ—‚ï¸ ProjectÂ Layout

```
â”œâ”€ api.py            # FastAPI server + FAISS search
â”œâ”€ llm.py            # Gemini wrapper
â”œâ”€ data/faq_chunks.pkl
â”œâ”€ models/           # Sentenceâ€‘Transformers embeddings
â”œâ”€ templates/        # (unused) Jinja templates
â””â”€ README.md
```

---

## ğŸ™Œ Contributing

1. Fork the repo & create a branch: `git checkout -b feat/my-feature`  
2. Commit changes with conventional commits  
3. Push and open a PR â€“ feedback is welcome!

---

## ğŸ“„ License

This project is released under the [MIT License](LICENSE).

---

### âœ‰ï¸ Contact

Maintainer â€“ **KenÂ Su** Â· [GitHubÂ @KenSu223](https://github.com/KenSu223)